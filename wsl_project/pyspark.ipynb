{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b17f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/05 13:05:16 WARN Utils: Your hostname, DESKTOP-VV6FGUQ, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/05 13:05:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/05 13:05:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>wsl-ui-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e30f713c320>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"wsl-ui-test\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.ui.port\", \"4046\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Djava.net.preferIPv4Stack=true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82116e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', True).option('inferSchema', True).csv(\"/home/ilyasius/pyspark_projects/data_emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c343a1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda85a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def git(df, n=100, fmt=\"fancy_grid\"):\n",
    "    data = df.limit(n).collect()\n",
    "    headers = df.columns\n",
    "    print(tabulate(data, headers=headers, tablefmt=fmt, showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21277de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤══════════════╤═════════════╤═════════════════════════╤══════════════╤══════════╤════════════════╤═══════╕\n",
      "│   employee_id │ first_name   │ last_name   │ email                   │ department   │   salary │ joining_date   │   age │\n",
      "╞═══════════════╪══════════════╪═════════════╪═════════════════════════╪══════════════╪══════════╪════════════════╪═══════╡\n",
      "│             1 │ Joshua       │ Ramos       │ alexandra29@gmail.com   │ Operations   │  65313.8 │ 23-05-2024     │    42 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             2 │ Christina    │ Clark       │ littlekyle@yahoo.com    │ IT           │  58827.3 │ 02-10-2021     │    46 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             3 │ Jonathon     │ Sullivan    │ tannerivan@johnson.com  │ Operations   │  57427.3 │ 30-04-2020     │    50 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             4 │ Clinton      │ Aguirre     │ jamieortega@yahoo.com   │ HR           │  45352.7 │ 04-04-2021     │    55 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             5 │ Veronica     │ Weber       │ trevinosteven@ramos.com │ Operations   │  78772.6 │ 06-10-2020     │    51 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             6 │ Louis        │ Rivera      │ parksamy@gmail.com      │ Finance      │  30448.7 │ 05-05-2020     │    41 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             7 │ Sabrina      │ Gonzalez    │ rwillis@mason-clark.biz │ Marketing    │  88476.7 │ 15-10-2020     │    48 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             8 │ Jillian      │ Lewis       │ sreese@mclean-baker.net │ Marketing    │  49895.6 │ 08-06-2021     │    33 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│             9 │ Caroline     │ Kennedy     │ inguyen@huynh.com       │ Finance      │  88878.6 │ 17-07-2024     │    47 │\n",
      "├───────────────┼──────────────┼─────────────┼─────────────────────────┼──────────────┼──────────┼────────────────┼───────┤\n",
      "│            10 │ Adam         │ Sanchez     │ luisclark@hotmail.com   │ HR           │  88331.2 │ 23-03-2020     │    38 │\n",
      "╘═══════════════╧══════════════╧═════════════╧═════════════════════════╧══════════════╧══════════╧════════════════╧═══════╛\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- joining_date: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git(df,10)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba8ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"click\", 10, datetime.strptime(\"2023-01-01 10:00\", \"%Y-%m-%d %H:%M\")),\n",
    "    (1, \"click\", 20, datetime.strptime(\"2023-01-01 10:05\", \"%Y-%m-%d %H:%M\")),\n",
    "    (1, \"buy\", 100, datetime.strptime(\"2023-01-01 10:10\", \"%Y-%m-%d %H:%M\")),\n",
    "    (2, \"click\", 5, datetime.strptime(\"2023-01-01 11:00\", \"%Y-%m-%d %H:%M\")),\n",
    "    (2, \"buy\", 50, datetime.strptime(\"2023-01-01 11:02\", \"%Y-%m-%d %H:%M\"))\n",
    "]\n",
    "\n",
    "df_1 = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb8e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef0c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-------------------+\n",
      "|user_id|event|value|                 ts|\n",
      "+-------+-----+-----+-------------------+\n",
      "|      1|click|   10|2023-01-01 10:00:00|\n",
      "|      1|click|   20|2023-01-01 10:05:00|\n",
      "|      1|  buy|  100|2023-01-01 10:10:00|\n",
      "|      2|click|    5|2023-01-01 11:00:00|\n",
      "|      2|  buy|   50|2023-01-01 11:02:00|\n",
      "+-------+-----+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff52810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤═══════╤═════════╕\n",
      "│   user_id │   buy │   click │\n",
      "╞═══════════╪═══════╪═════════╡\n",
      "│         1 │     1 │       2 │\n",
      "├───────────┼───────┼─────────┤\n",
      "│         2 │     1 │       1 │\n",
      "╘═══════════╧═══════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "df = df_1.groupBy('user_id')\\\n",
    "    .pivot('event')\\\n",
    "    .count()\n",
    "git(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15befb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"ts\", StringType(), True) \n",
    "])\n",
    "\n",
    "data1 = [\n",
    "    (1, 101, 10, \"2023-01-01 10:00\"),\n",
    "    (1, 102, 20, \"2023-01-01 10:05\"),\n",
    "    (1, 103, 30, \"2023-01-01 10:10\"),\n",
    "    (2, 201, 50, \"2023-01-01 11:00\"),\n",
    "    (2, 202, 70, \"2023-01-01 11:05\")\n",
    "]\n",
    "\n",
    "per = spark.createDataFrame(data1, schema1)\n",
    "\n",
    "df_2 = per.withColumn(\"ts\", per[\"ts\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa0cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------------+\n",
      "|user_id|order_id|amount|                 ts|\n",
      "+-------+--------+------+-------------------+\n",
      "|      1|     101|    10|2023-01-01 10:00:00|\n",
      "|      1|     102|    20|2023-01-01 10:05:00|\n",
      "|      1|     103|    30|2023-01-01 10:10:00|\n",
      "|      2|     201|    50|2023-01-01 11:00:00|\n",
      "|      2|     202|    70|2023-01-01 11:05:00|\n",
      "+-------+--------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6418b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤════════════╤══════════╤═════════════════════╤═════════════╕\n",
      "│   user_id │   order_id │   amount │ ts                  │   total_pay │\n",
      "╞═══════════╪════════════╪══════════╪═════════════════════╪═════════════╡\n",
      "│         1 │        101 │       10 │ 2023-01-01 10:00:00 │          10 │\n",
      "├───────────┼────────────┼──────────┼─────────────────────┼─────────────┤\n",
      "│         1 │        102 │       20 │ 2023-01-01 10:05:00 │          30 │\n",
      "├───────────┼────────────┼──────────┼─────────────────────┼─────────────┤\n",
      "│         1 │        103 │       30 │ 2023-01-01 10:10:00 │          60 │\n",
      "├───────────┼────────────┼──────────┼─────────────────────┼─────────────┤\n",
      "│         2 │        201 │       50 │ 2023-01-01 11:00:00 │          50 │\n",
      "├───────────┼────────────┼──────────┼─────────────────────┼─────────────┤\n",
      "│         2 │        202 │       70 │ 2023-01-01 11:05:00 │         120 │\n",
      "╘═══════════╧════════════╧══════════╧═════════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "w=Window.partitionBy('user_id').orderBy('ts')\n",
    "df = df_2.withColumn('total_pay', sum('amount').over(w))\n",
    "git(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817b2419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+\n",
      "|user_id| event|                 ts|\n",
      "+-------+------+-------------------+\n",
      "|      1| login|2023-01-01 10:00:00|\n",
      "|      1| click|2023-01-01 10:01:00|\n",
      "|      1|logout|2023-01-01 10:05:00|\n",
      "|      2| login|2023-01-01 11:00:00|\n",
      "|      2|logout|2023-01-01 11:02:00|\n",
      "+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema2 = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "data2 = [\n",
    "    (1, \"login\", \"2023-01-01 10:00\"),\n",
    "    (1, \"click\", \"2023-01-01 10:01\"),\n",
    "    (1, \"logout\", \"2023-01-01 10:05\"),\n",
    "    (2, \"login\", \"2023-01-01 11:00\"),\n",
    "    (2, \"logout\", \"2023-01-01 11:02\")\n",
    "]\n",
    "\n",
    "per1 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "df_3 = per1.withColumn(\"ts\", per1[\"ts\"].cast(\"timestamp\"))\n",
    "\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ecb1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤════════════════╕\n",
      "│   user_id │   time_session │\n",
      "╞═══════════╪════════════════╡\n",
      "│         1 │            300 │\n",
      "├───────────┼────────────────┤\n",
      "│         2 │            120 │\n",
      "╘═══════════╧════════════════╛\n"
     ]
    }
   ],
   "source": [
    "df_filter = df_3.filter(col('event').isin('login','logout'))\n",
    "\n",
    "w=Window.partitionBy('user_id').orderBy('ts')\n",
    "\n",
    "df_result = df_filter.withColumn('lag_ts', lag('ts').over(w))\\\n",
    "        .withColumn('time_session', col('ts').cast('long') - col('lag_ts').cast('long'))\\\n",
    "        .select('user_id', 'time_session')\\\n",
    "        .dropna()\n",
    "git(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b80512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|country|\n",
      "+-------+-------+\n",
      "|      1|     US|\n",
      "|      2|     UK|\n",
      "|      3|     US|\n",
      "+-------+-------+\n",
      "\n",
      "+-------+------+----------+\n",
      "|user_id|amount|        ts|\n",
      "+-------+------+----------+\n",
      "|      1|   100|2026-01-01|\n",
      "|      2|    50|2026-01-02|\n",
      "|      1|   150|2026-01-03|\n",
      "|      3|   200|2023-01-04|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema3 = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "data3 = [(1, \"US\"), (2, \"UK\"), (3, \"US\")]\n",
    "df_country = spark.createDataFrame(data3, schema3)\n",
    "\n",
    "\n",
    "schema4 = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"ts\", StringType(), True)\n",
    "])\n",
    "data4 = [\n",
    "    (1, 100, \"2026-01-01\"),\n",
    "    (2, 50,  \"2026-01-02\"),\n",
    "    (1, 150, \"2026-01-03\"),\n",
    "    (3, 200, \"2023-01-04\")\n",
    "]\n",
    "df_transactions = spark.createDataFrame(data4, schema4)\n",
    "df_transactions = df_transactions.withColumn(\"ts\", col('ts').cast('date'))\n",
    "\n",
    "\n",
    "df_country.show()\n",
    "df_transactions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ec4a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤═════════════════════════════╕\n",
      "│ country   │   avg_amount_country_30_day │\n",
      "╞═══════════╪═════════════════════════════╡\n",
      "│ US        │                         125 │\n",
      "├───────────┼─────────────────────────────┤\n",
      "│ UK        │                          50 │\n",
      "╘═══════════╧═════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "df_fl = df_transactions.filter(col('ts') > date_sub(current_date(), 30))\n",
    "df_result = df_fl.join(df_country, on='user_id',how='inner')\\\n",
    "    .groupBy('country')\\\n",
    "    .agg(avg('amount').alias('avg_amount_country_30_day'))\n",
    "\n",
    "git(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce3c3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-------------------+\n",
      "|user_id| event|value|                 ts|\n",
      "+-------+------+-----+-------------------+\n",
      "|      1| click|   10|2023-01-01 10:00:00|\n",
      "|      1|   buy|  100|2023-01-01 10:10:00|\n",
      "|      1|refund|   50|2023-01-01 10:15:00|\n",
      "|      2| click|    5|2023-01-01 11:00:00|\n",
      "|      2|   buy|  200|2023-01-01 11:05:00|\n",
      "|      2|refund|   20|2023-01-01 11:10:00|\n",
      "+-------+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True),\n",
    "    StructField(\"ts\", StringType(), True)  \n",
    "])\n",
    "\n",
    "# Исходные данные\n",
    "data = [\n",
    "    (1, \"click\", 10, \"2023-01-01 10:00\"),\n",
    "    (1, \"buy\", 100, \"2023-01-01 10:10\"),\n",
    "    (1, \"refund\", 50, \"2023-01-01 10:15\"),\n",
    "    (2, \"click\", 5, \"2023-01-01 11:00\"),\n",
    "    (2, \"buy\", 200, \"2023-01-01 11:05\"),\n",
    "    (2, \"refund\", 20, \"2023-01-01 11:10\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"ts\",\n",
    "    to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm\")\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e5857b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+------+\n",
      "|user_id|buy|click|refund|\n",
      "+-------+---+-----+------+\n",
      "|      1|  1|    1|     1|\n",
      "|      2|  1|    1|     1|\n",
      "+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('user_id').pivot('event').agg(count(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09cb8ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+\n",
      "|user_id|amount|                 ts|\n",
      "+-------+------+-------------------+\n",
      "|      1|    10|2023-01-01 10:00:00|\n",
      "|      1|    15|2023-01-01 10:05:00|\n",
      "|      1|    20|2023-01-01 10:10:00|\n",
      "|      2|     5|2023-01-02 09:00:00|\n",
      "|      2|     7|2023-01-02 09:10:00|\n",
      "+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"ts\", StringType(), True) \n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 10, \"2023-01-01 10:00\"),\n",
    "    (1, 15, \"2023-01-01 10:05\"),\n",
    "    (1, 20, \"2023-01-01 10:10\"),\n",
    "    (2, 5,  \"2023-01-02 09:00\"),\n",
    "    (2, 7,  \"2023-01-02 09:10\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"ts\",\n",
    "    to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7736308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+----------+--------------+\n",
      "|user_id|first_amount|last_amount|sum_amount|purchase_count|\n",
      "+-------+------------+-----------+----------+--------------+\n",
      "|      1|          10|         20|        45|             3|\n",
      "|      2|           5|          7|        12|             2|\n",
      "+-------+------------+-----------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w=Window.partitionBy('user_id').orderBy('ts')\n",
    "w1=Window.partitionBy('user_id').orderBy(desc('ts'))\n",
    "\n",
    "df_create = df.withColumn('rn', row_number().over(w))\\\n",
    "        .withColumn('rn2', row_number().over(w1))\\\n",
    "\n",
    "df_rn = df_create.filter(col('rn') == 1).select('user_id', col('amount').alias('first_amount'))\n",
    "\n",
    "df_rn2 = df_create.filter(col('rn2') == 1).select('user_id', col('amount').alias('last_amount'))\n",
    "\n",
    "df_sum = df.groupBy('user_id')\\\n",
    "        .agg(sum('amount').alias('sum_amount'), \n",
    "        count(\"*\").alias('purchase_count'))\n",
    "        \n",
    "df_result = df_rn.join(df_rn2, on='user_id', how='inner')\\\n",
    "        .join(df_sum, on='user_id', how='inner')\n",
    "\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3c077ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------------------+\n",
      "|order_id|user_id|items                  |\n",
      "+--------+-------+-----------------------+\n",
      "|1       |10     |[{100, 50}, {101, 70}] |\n",
      "|2       |20     |[{100, 50}]            |\n",
      "|3       |10     |[{103, 120}, {104, 90}]|\n",
      "+--------+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"item_id\", IntegerType(), True),\n",
    "        StructField(\"price\", IntegerType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"order_id\": 1,\n",
    "        \"user_id\": 10,\n",
    "        \"items\": [\n",
    "            {\"item_id\": 100, \"price\": 50},\n",
    "            {\"item_id\": 101, \"price\": 70}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"order_id\": 2,\n",
    "        \"user_id\": 20,\n",
    "        \"items\": [\n",
    "            {\"item_id\": 100, \"price\": 50}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"order_id\": 3,\n",
    "        \"user_id\": 10,\n",
    "        \"items\": [\n",
    "            {\"item_id\": 103, \"price\": 120},\n",
    "            {\"item_id\": 104, \"price\": 90}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fff24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-----+--------------------------+\n",
      "|order_id|user_id|item_id|price|cur                       |\n",
      "+--------+-------+-------+-----+--------------------------+\n",
      "|1       |10     |100    |50   |2025-12-05 13:06:00.670271|\n",
      "|1       |10     |101    |70   |2025-12-05 13:06:00.670271|\n",
      "|2       |20     |100    |50   |2025-12-05 13:06:00.670271|\n",
      "|3       |10     |103    |120  |2025-12-05 13:06:00.670271|\n",
      "|3       |10     |104    |90   |2025-12-05 13:06:00.670271|\n",
      "+--------+-------+-------+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('order_id','user_id', explode(col('items')).alias('items'))\\\n",
    "    .withColumn('item_id', col('items.item_id'))\\\n",
    "    .withColumn('price', col('items.price'))\\\n",
    "    .withColumn('cur', current_timestamp())\\\n",
    "    .drop('items')\\\n",
    "    .show(truncate=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4920bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"ts\", StringType(), True)  \n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"click\", \"2023-01-01 10:00\"),\n",
    "    (1, \"click\", \"2023-01-01 10:05\"),\n",
    "    (1, \"click\", \"2023-01-01 10:40\"),\n",
    "    (1, \"click\", \"2023-01-01 10:55\"),\n",
    "    (2, \"click\", \"2023-01-01 11:00\"),\n",
    "    (2, \"click\", \"2023-01-01 11:20\"),\n",
    "    (2, \"click\", \"2023-01-01 12:00\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"ts\", to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fdb52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+\n",
      "|user_id|event|                 ts|\n",
      "+-------+-----+-------------------+\n",
      "|      1|click|2023-01-01 10:00:00|\n",
      "|      1|click|2023-01-01 10:05:00|\n",
      "|      1|click|2023-01-01 10:40:00|\n",
      "|      1|click|2023-01-01 10:55:00|\n",
      "|      2|click|2023-01-01 11:00:00|\n",
      "|      2|click|2023-01-01 11:20:00|\n",
      "|      2|click|2023-01-01 12:00:00|\n",
      "+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9882e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------------+-------------------+------------+\n",
      "|user_id|enumerate|      start_session|        end_session|time_session|\n",
      "+-------+---------+-------------------+-------------------+------------+\n",
      "|      1|        1|2023-01-01 10:00:00|2023-01-01 10:05:00|         5.0|\n",
      "|      1|        2|2023-01-01 10:40:00|2023-01-01 10:55:00|        15.0|\n",
      "|      2|        1|2023-01-01 11:00:00|2023-01-01 11:20:00|        20.0|\n",
      "|      2|        2|2023-01-01 12:00:00|2023-01-01 12:00:00|         0.0|\n",
      "+-------+---------+-------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w=Window.partitionBy('user_id').orderBy('ts')\n",
    "\n",
    "df1 = df.withColumn('lag_ts', lag('ts').over(w))\\\n",
    "        .withColumn('enumerate', \n",
    "            when(\n",
    "            col('lag_ts').isNull() | (col('ts').cast('long') - col('lag_ts').cast('long') > 1800),1\n",
    "            ).otherwise(0)\n",
    "        )\\\n",
    "        .withColumn('enumerate', sum('enumerate').over(w))\\\n",
    "        .drop('lag_ts')\n",
    "\n",
    "w1=Window.partitionBy('user_id','enumerate')\n",
    "\n",
    "df2 = df1.withColumn('start_session', min('ts').over(w1))\\\n",
    "        .withColumn('end_session', max('ts').over(w1))\\\n",
    "        .withColumn('time_session', (col('end_session').cast('long') - col('start_session').cast('long')) / 60)\\\n",
    "        .drop('event', 'ts')\\\n",
    "        .distinct()\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a95afb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤════════════╕\n",
      "│   user_id │ date       │\n",
      "╞═══════════╪════════════╡\n",
      "│         1 │ 2024-01-01 │\n",
      "├───────────┼────────────┤\n",
      "│         1 │ 2024-01-02 │\n",
      "├───────────┼────────────┤\n",
      "│         1 │ 2024-01-03 │\n",
      "├───────────┼────────────┤\n",
      "│         1 │ 2024-01-10 │\n",
      "├───────────┼────────────┤\n",
      "│         2 │ 2024-02-01 │\n",
      "├───────────┼────────────┤\n",
      "│         2 │ 2024-02-03 │\n",
      "├───────────┼────────────┤\n",
      "│         2 │ 2024-02-04 │\n",
      "╘═══════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"2024-01-01\"),\n",
    "    (1, \"2024-01-02\"),\n",
    "    (1, \"2024-01-03\"),\n",
    "    (1, \"2024-01-10\"),\n",
    "    (2, \"2024-02-01\"),\n",
    "    (2, \"2024-02-03\"),\n",
    "    (2, \"2024-02-04\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"date\"])\n",
    "df = df.withColumn(\"date\", to_date(\"date\"))\n",
    "git(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33d1b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[user_id#911L, enumerate#916L, time_active#918L], functions=[])\n",
      "   +- HashAggregate(keys=[user_id#911L, enumerate#916L, time_active#918L], functions=[])\n",
      "      +- Window [count(1) windowspecdefinition(user_id#911L, enumerate#916L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS time_active#918L], [user_id#911L, enumerate#916L]\n",
      "         +- Sort [user_id#911L ASC NULLS FIRST, enumerate#916L ASC NULLS FIRST], false, 0\n",
      "            +- Project [user_id#911L, enumerate#916L]\n",
      "               +- Window [sum(enumerate#915) windowspecdefinition(user_id#911L, date#913 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS enumerate#916L], [user_id#911L], [date#913 ASC NULLS FIRST]\n",
      "                  +- Project [user_id#911L, date#913, CASE WHEN (isnull(lag_date#914) OR (datediff(date#913, lag_date#914) > 1)) THEN 1 ELSE 0 END AS enumerate#915]\n",
      "                     +- Window [lag(date#913, -1, null) windowspecdefinition(user_id#911L, date#913 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag_date#914], [user_id#911L], [date#913 ASC NULLS FIRST]\n",
      "                        +- Sort [user_id#911L ASC NULLS FIRST, date#913 ASC NULLS FIRST], false, 0\n",
      "                           +- Exchange hashpartitioning(user_id#911L, 200), ENSURE_REQUIREMENTS, [plan_id=3171]\n",
      "                              +- Project [user_id#911L, cast(date#912 as date) AS date#913]\n",
      "                                 +- Scan ExistingRDD[user_id#911L,date#912]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w=Window.partitionBy('user_id').orderBy('date')\n",
    "\n",
    "df1 = df.withColumn('lag_date', lag('date').over(w))\\\n",
    "    .withColumn('enumerate',\n",
    "                when(\n",
    "                    col('lag_date').isNull() | (datediff(col('date'), col('lag_date')) > 1), 1\n",
    "                    ).otherwise(0)\n",
    "                )\\\n",
    "    .withColumn('enumerate', sum('enumerate').over(w))\\\n",
    "    .drop('lag_date','date')\n",
    "    \n",
    "w1=Window.partitionBy('user_id','enumerate')\n",
    "\n",
    "df_result = df1.withColumn('time_active', count(\"*\").over(w1)).distinct()\n",
    "\n",
    "df_result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8a66deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|score|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  2| NULL|\n",
      "|  3|    5|\n",
      "|  4| NULL|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 10),\n",
    "    (2, None),\n",
    "    (3, 5),\n",
    "    (4, None)\n",
    "    ], schema=['id', 'score'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb2f6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════╤═════════╕\n",
      "│   id │   score │\n",
      "╞══════╪═════════╡\n",
      "│    1 │      10 │\n",
      "├──────┼─────────┤\n",
      "│    2 │       7 │\n",
      "├──────┼─────────┤\n",
      "│    3 │       5 │\n",
      "├──────┼─────────┤\n",
      "│    4 │       7 │\n",
      "╘══════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "avgg = df.select(avg('score')).take(1)[0][0]\n",
    "\n",
    "df = df.fillna(avgg)\n",
    "\n",
    "git(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7916212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedstarwithcolumns(date, 'to_date('date), None)]\n",
      "+- LogicalRDD [user_id#911L, date#912], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "user_id: bigint, date: date\n",
      "Project [user_id#911L, to_date(date#912, None, Some(Asia/Yekaterinburg), true) AS date#913]\n",
      "+- LogicalRDD [user_id#911L, date#912], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [user_id#911L, cast(date#912 as date) AS date#913]\n",
      "+- LogicalRDD [user_id#911L, date#912], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [user_id#911L, cast(date#912 as date) AS date#913]\n",
      "+- *(1) Scan ExistingRDD[user_id#911L,date#912]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac1838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b10cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23327d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf90e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246a2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea599cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b61ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df0286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35ece8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2f826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e740b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2732b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23093d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (pyspark_venv)",
   "language": "python",
   "name": "pyspark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
